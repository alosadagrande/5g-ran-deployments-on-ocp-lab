= Deploying an Image Based Install Cluster
include::_attributes.adoc[]
:profile: 5g-ran-lab

[#introduction]
== Introduction

In the previous section, link:creating-seed-image.html[Creating the Seed Image], we created a seed image of a Single Node OpenShift cluster version {sno-cluster-version1}. That seed image will be used to deploy a new SNO in less than 20 minutes, that's why, specially in telco RAN environments where we are managing thousands of clusters , this deployment method is the preferred one.

The following picture describe the process we are going to follow to deploy and configure the *sno3* OpenShift Clusters with the RAN DU profile configured:

image::ibi-installation.png[ZTP Workflow 2]

* A seed image is already created and stored in our link:lab-environment-introduction.html#container-registry[container registry].
* The proper RHCOS images are available in a link:lab-environment-introduction.html#webserver[HTTP server] deployed in the infrastructure server.
* The ImageBasedInstallationConfig CR is provided and will be used by the `openshift-install` binary to create the installation ISO.
* The installation ISO is mounted remotely to the sno3 and the preinstallation process starts.

Once the preinstallation is finished, the sno3 will be shutdown. At this point, in a telco RAN scenario, the thousands of SNO clusters that were preinstalled in the customer premises will be transported to the remote locations.  In this lab, once the sno3 arrives to the remote location, the SNO is ready to be reconfigured and complete the installation.

* The Image Based Install (IBI) Operator running in the hub cluster takes the site-specific configuration resources (`ImageClusterInstall`, `ClusterDeployment`, `BaremetalHost`, etc) and creates the Configuration ISO.
* The configuration ISO is mounted in the sno3 and the host is booted.
* The Lifecycle-Agent Operator that was included in the seed image and so it is available in the sno3, detects the configuration ISO and starts the reconfiguration process until the OpenShift Cluster is completely deployed.

[#baremetal-node-details]
== Bare Metal Node Details

The details for our baremetal node that we want to provision as SNO3 are the ones below:

* RedFish Endpoint: `redfish-virtualmedia://192.168.125.1:9000/redfish/v1/Systems/local/sno3`
* MAC Address: `aa:aa:aa:aa:04:01`
* Primary disk: `/dev/vda`
* BMC User: `admin`
* BMC Password: `admin`

[#creating-the-installation-iso]
== Creating the Installation ISO

In order to create the installation ISO we are going to create a working directory in the infrastructure node.

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
mkdir -p ~/imagebaseinstall/ibi-iso-workdir
cd ~/imagebaseinstall/
-----

Let's create the `ImageBasedInstallationConfig`. Notice that the `seedImage` spec is pointing at the container registry where the seed image that we created was pushed.

[.console-input]
[source,bash,subs="attributes+"]
-----
cat <<EOF > ~/imagebaseinstall/ibi-iso-workdir/image-based-installation-config.yaml
---
apiVersion: v1beta1
kind: ImageBasedInstallationConfig
metadata:
  name: example-image-based-installation-config
seedImage: infra.5g-deployment.lab:8443/ibi/lab5gran:{sno-cluster-version1}
seedVersion: {sno-cluster-version1-cvo}
extraPartitionStart: "-60G"
installationDisk: "/dev/vda"
sshKey: "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQC5pFKFLOuxrd9Q/TRu9sRtwGg2PV+kl2MHzBIGUhCcR0LuBJk62XG9tQWPQYTQ3ZUBKb6pRTqPXg+cDu5FmcpTwAKzqgUb6ArnjECxLJzJvWieBJ7k45QzhlZPeiN2Omik5bo7uM/P1YIo5pTUdVk5wJjaMOb7Xkcmbjc7r22xY54cce2Wb7B1QDtLWJkq++eJHSX2GlEjfxSlEvQzTN7m2N5pmoZtaXpLKcbOqtuSQSVKC4XPgb57hgEs/ZZy/LbGGHZyLAW5Tqfk1JCTFGm6Q+oOd3wAOF1SdUxM7frdrN3UOB12u/E6YuAx3fDvoNZvcrCYEpjkfrsjU91oz78aETZV43hOK9NWCOhdX5djA7G35/EMn1ifanVoHG34GwNuzMdkb7KdYQUztvsXIC792E2XzWfginFZha6kORngokZ2DwrzFj3wgvmVyNXyEOqhwi6LmlsYdKxEvUtiYhdISvh2Y9GPrFcJ5DanXe7NVAKXe5CyERjBnxWktqAPBzXJa36FKIlkeVF5G+NWgufC6ZWkDCD98VZDiPP9sSgqZF8bSR4l4/vxxAW4knKIZv11VX77Sa1qZOR9Ml12t5pNGT7wDlSOiDqr5EWsEexga/2s/t9itvfzhcWKt+k66jd8tdws2dw6+8JYJeiBbU63HBjxCX+vCVZASrNBjiXhFw=="
pullSecret: '{"auths": {"infra.5g-deployment.lab:8443": {"auth": "YWRtaW46cjNkaDR0MSE="}}}'
shutdown: true
imageDigestSources:
  - mirrors:
    - infra.5g-deployment.lab:8443/prega/test/multicluster-engine
    source: registry.redhat.io/multicluster-engine
  - mirrors:
    - infra.5g-deployment.lab:8443/prega/test/openshift-gitops-1
    source: registry.redhat.io/openshift-gitops-1
  - mirrors:
    - infra.5g-deployment.lab:8443/prega/test/rh-sso-7
    source: registry.redhat.io/rh-sso-7
  - mirrors:
    - infra.5g-deployment.lab:8443/prega/test/lvms4
    source: registry.redhat.io/lvms4
  - mirrors:
    - infra.5g-deployment.lab:8443/prega/test/openshift4
    source: registry.redhat.io/openshift4
  - mirrors:
    - infra.5g-deployment.lab:8443/prega/test/rhacm2
    source: registry.redhat.io/rhacm2
  - mirrors:
    - infra.5g-deployment.lab:8443/prega/test/rhel8
    source: registry.redhat.io/rhel8
  - mirrors:
    - infra.5g-deployment.lab:8443/prega/test/oadp
    source: registry.redhat.io/oadp
  - mirrors:
    - infra.5g-deployment.lab:8443/prega/test/rh-osbs
    source: quay.io/prega/test/rh-osbs
  - mirrors:
    - infra.5g-deployment.lab:8443/openshift-release-dev/ocp-v4.0-art-dev
    - infra.5g-deployment.lab:8443/openshift/release
    source: quay.io/openshift-release-dev/ocp-v4.0-art-dev
  - mirrors:
    - infra.5g-deployment.lab:8443/openshift-release-dev
    source: quay.io/openshift-release-dev
  - mirrors:
    - infra.5g-deployment.lab:8443/openshift/release-images
    source: quay.io/openshift-release-dev/ocp-release
networkConfig:
    interfaces:
      - name: enp3s0
        type: ethernet
        state: up
        ipv4:
          enabled: true
          dhcp: true
          auto-dns: true
        ipv6:
          enabled: false
    routes:
      config:
      - destination: 0.0.0.0/0
        metric: 150
        next-hop-address: 192.168.125.1
        next-hop-interface: enp3s0
additionalTrustBundle: |
  -----BEGIN CERTIFICATE-----
  MIIFSzCCAzOgAwIBAgIUEz9Cc3eC77zRr/y6Eg7wPLzjYBswDQYJKoZIhvcNAQEL
  BQAwIjEgMB4GA1UEAwwXaW5mcmEuNWctZGVwbG95bWVudC5sYWIwIBcNMjIxMTIx
  MTcwMDIwWhgPMjA1MDA0MDcxNzAwMjBaMCIxIDAeBgNVBAMMF2luZnJhLjVnLWRl
  cGxveW1lbnQubGFiMIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEAm/dG
  FbJkusaOpMlDJ9GJ1+Z3Y2G0/RxVRuqZ5E7ufZk69vp9DaERclN/KwiTNMoRUvcZ
  gtIj2HAfaN8L7FfwN8qqMfeJg/fdh9X0sqNydMaZ/fSA60LxmUqmU4e7g01obTmH
  e6j2vjRprDDzfE1683VvgWkecroeG4mujHUUREHs9xfgoW7+nHDpKWZ1vLaZdFKy
  Zrzz1IBvPo+DxgL+3n8orb0aM3uQdufQ2uHZ2fYrxrKUzIVFv+CTT7ctEG/PjQxJ
  tjswizCG6Obk9+B3CMp/s6mT6W7P9xzpbW3aRZLMxSR3/lSAxpWTP/57G3ZyXoNJ
  Cp2UtZEyxRGG1M0f3epzvu4H8JYuXTs/+w6JRLJTN7BF+dThuDKCmbkzq7NzJ/Kq
  ln3qEvFVgMKLprkY7hzU/e/Egr6QA26c2nvwJ2vV5COJrqaPSq52NubseVGPnK2s
  kkKWdf4wPwE1/LrbcjxpcUwJysy+oOmowYhx8X8GUUBZk8ejupYkg/gGop2F9JWD
  sOwmWqRBqn7yKJF4GyJZ0h62xhEfQdBHKub6VLfh6GFNrNHvNpy+DFq3nWGKnV7j
  y5wxx2bR5exN/qZ8wyaFq5k9tVLFd1CMAzQkWkmT9EpI6y4Ux9tPGXgB6h6yjjnK
  gxbLH84ejwDaaiSc2NBVP+47b7Vhoiw4++hNBqcCAwEAAaN3MHUwHQYDVR0OBBYE
  FGQeCqTJ5HZJIXNKx2t7dY+fRYo1MB8GA1UdIwQYMBaAFGQeCqTJ5HZJIXNKx2t7
  dY+fRYo1MA8GA1UdEwEB/wQFMAMBAf8wIgYDVR0RBBswGYIXaW5mcmEuNWctZGVw
  bG95bWVudC5sYWIwDQYJKoZIhvcNAQELBQADggIBAEvsOv4uVMHUZrFQrwUYJRV7
  MC/FB0bgZZ1VhqpL+7+W98+HEYVZRuK9IKjGRfen9wOrLI6hc6zZYOwMDJgfuFwd
  X6qv/HZKp+TfZrFu1IhgDeTPkJX7t7ECD63BgOSNc8OgmGL34dP5qCB3qaSzuP9x
  mukIAZyvHwf2ZfWzrpvR9GLzTuh01GHqQyojC9ntWvlzgKec3nZNFt8tWRyMraAr
  C/a++HeOlZCeRtH9gSOy49H1B7/kfTwFw/Y/h0oVMpH6x8dewyuefe8Q5fvURO3T
  y4B/esUA9R/h971BGIoYk5pAZAJdkD8GAmegyj47vFg6mw095dwB1eNAD7ddqdQn
  RCqwrqYEV1TExI23mC0oiDck0RY8FWI+Q034MOnZFn6Dv6EYMF4IBjJIJICMqvSf
  MA/AXZ111P5/5j+qODTwJ/IDhiT46HMY/SN3MW96uZuJKchJchMNQG+MOuTJb6gd
  cVVyItPgufANPzlf4GpF0+OaOMRjg2BdeRJKluWhie1rSKT/DpvB9ZBWU6ng/MFa
  oW5xpMLuZIUF45kP2ZhQhbRA2zjIaZ8XPgaHPNr4INhSW5pqqLISZCJvkMJV07eT
  s+KzXHlydQpzajOOzwRgq+dIGl6y4GYM1Y0EElHY7S/LvJBNpcw8BDjWmZfpie0y
  /fp7Z8v3m9S2+TmGjDVC
  -----END CERTIFICATE-----
EOF
-----

Create the Installation ISO by running the following command:

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
~/openshift-install image-based create image --dir ibi-iso-workdir
-----

Verify that the rhcos-ibi.iso image is created in our working directory:

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
 ls -l ~/imagebaseinstall/ibi-iso-workdir
-----
[.console]
[source,bash,subs="attributes+,+macros"]
-----
total 1250304
-rw-r--r--. 1 root root 1280311296 Feb 25 12:05 rhcos-ibi.iso
-----

At this stage we are ready to manually boot the sno3 with the Installation ISO we created. But, first we have to move the ISO to a place where the host about to install can grab it. Let's copy it to the link:lab-environment-introduction.html#webserver[HTTP server] we have available in our infrastructure:

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
cp ~/imagebaseinstall/ibi-iso-workdir/rhcos-ibi.iso /opt/webcache/data/
-----

[#preinstallation-process]
== Pre-Installing the Host

Now we are ready to boot sno3 with the Installation ISO. Run the following command from the infrastructure host:

IMPORTANT: We are using kcli tool to boot the server from the ISO by leveraging the RedFish API. You can boot the server following other methods at your disposal.

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
kcli start baremetal -u admin -p admin -P iso_url=http://192.168.125.1:8080/rhcos-ibi.iso https://192.168.125.1:9000/redfish/v1/Systems/local/sno3
-----

Then, the sno3 will boot and the preinstallation process will start. We can take a look at the process by connecting to the sno3 via ssh. Something similar to the following output will be shown in the terminal. 

WARNING: The pre-installation process is finished once the sno3 is automatically powered off.

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
ssh -i ~/.ssh/snokey core@192.168.125.50
-----

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
journalctl -f
-----

[console]
[source,bash,subs="attributes+,+macros"]
-----
...
Feb 04 10:37:52 ocp-sno3 systemd[1]: Starting SNO Image-based Installation...
Feb 04 10:37:52 ocp-sno3 systemd[1]: iscsi.service: Unit cannot be reloaded because it is inactive.
Feb 04 10:37:53 ocp-sno3 systemd[1]: var-lib-containers-storage-overlay-opaque\x2dbug\x2dcheck2861559422-merged.mount: Deactivated successfully.
Feb 04 10:37:53 ocp-sno3 podman[1625]: 2025-02-04 10:37:53.120804852 +0000 UTC m=+0.201463935 system refresh
Feb 04 10:37:53 ocp-sno3 install-rhcos-and-restore-seed.sh[1625]: Trying to pull infra.5g-deployment.lab:8443/ibi/lab5gran:v4.18.0-rc.7...
Feb 04 10:37:53 ocp-sno3 install-rhcos-and-restore-seed.sh[1625]: Getting image source signatures
Feb 04 10:37:53 ocp-sno3 install-rhcos-and-restore-seed.sh[1625]: Copying blob sha256:93570925bc4c04f7a453bd0eb530ddad84ea634c54defef1ec2a6410ad1eb683
...
Feb 04 10:38:06 ocp-sno3 install-rhcos-and-restore-seed.sh[1795]: time="2025-02-04 10:38:06" level=info msg="IBI preparation process has started"
Feb 04 10:38:06 ocp-sno3 install-rhcos-and-restore-seed.sh[1795]: time="2025-02-04 10:38:06" level=info msg="Start preparing disk"
Feb 04 10:38:06 ocp-sno3 install-rhcos-and-restore-seed.sh[1795]: time="2025-02-04 10:38:06" level=info msg="Cleaning up /dev/vda disk"
Feb 04 10:38:06 ocp-sno3 install-rhcos-and-restore-seed.sh[1795]: time="2025-02-04 10:38:06" level=info msg="Start cleaning up device /dev/vda"
Feb 04 10:38:06 ocp-sno3 install-rhcos-and-restore-seed.sh[1795]: time="2025-02-04 10:38:06" level=info msg="Executing vgs with args [--noheadings -o vg_name,pv_name]"
Feb 04 10:38:06 ocp-sno3 install-rhcos-and-restore-seed.sh[1795]: time="2025-02-04 10:38:06" level=info msg="Executing pvs with args [--noheadings -o pv_name]"
Feb 04 10:38:06 ocp-sno3 install-rhcos-and-restore-seed.sh[1795]: time="2025-02-04 10:38:06" level=info msg="Executing dmsetup with args [ls]"
Feb 04 10:38:06 ocp-sno3 install-rhcos-and-restore-seed.sh[1795]: time="2025-02-04 10:38:06" level=info msg="Executing mdadm with args [-v --query --detail --scan]"
Feb 04 10:38:06 ocp-sno3 install-rhcos-and-restore-seed.sh[1795]: time="2025-02-04 10:38:06" level=info msg="Executing wipefs with args [--all --force /dev/vda]"
Feb 04 10:38:06 ocp-sno3 install-rhcos-and-restore-seed.sh[1795]: time="2025-02-04 10:38:06" level=info msg="Writing image to disk"
Feb 04 10:38:06 ocp-sno3 install-rhcos-and-restore-seed.sh[1795]: time="2025-02-04 10:38:06" level=info msg="Executing coreos-installer with args [install /dev/vda]"
...
Feb 04 10:39:43 ocp-sno3 install-rhcos-and-restore-seed.sh[1795]: time="2025-02-04 10:39:43" level=info msg="Precaching imaging"
Feb 04 10:39:43 ocp-sno3 install-rhcos-and-restore-seed.sh[1795]: time="2025-02-04 10:39:43" level=info msg="Path doesn't exist, skipping chrootpath/host"
Feb 04 10:39:43 ocp-sno3 install-rhcos-and-restore-seed.sh[1795]: time="2025-02-04T10:39:43Z" level=info msg="Will attempt to pull 84 images"
Feb 04 10:39:43 ocp-sno3 install-rhcos-and-restore-seed.sh[1795]: time="2025-02-04T10:39:43Z" level=info msg="Configured precaching job to concurrently pull 10 images."
...
Feb 04 10:41:22 ocp-sno3 install-rhcos-and-restore-seed.sh[1795]: time="2025-02-04T10:41:22Z" level=info msg="Completed executing pre-caching"
Feb 04 10:41:22 ocp-sno3 install-rhcos-and-restore-seed.sh[1795]: time="2025-02-04T10:41:22Z" level=info msg="Failed to pre-cache the following images:"
Feb 04 10:41:22 ocp-sno3 systemd[1]: var-lib-containers-storage-overlay.mount: Deactivated successfully.
Feb 04 10:41:22 ocp-sno3 install-rhcos-and-restore-seed.sh[1795]: time="2025-02-04T10:41:22Z" level=info msg="quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:bcb68cf733405788242e599946ae51e3369edd3ceb6dd57a2a07531d75267f23, but found locally after downloading other images"
Feb 04 10:41:22 ocp-sno3 install-rhcos-and-restore-seed.sh[1795]: time="2025-02-04T10:41:22Z" level=info msg="Pre-cached images successfully."
Feb 04 10:41:22 ocp-sno3 install-rhcos-and-restore-seed.sh[1795]: time="2025-02-04 10:41:22" level=info msg="Executing ostree with args [admin undeploy --sysroot /mnt 1]"
Feb 04 10:41:22 ocp-sno3 ostree[5337]: Starting syncfs() for system root
Feb 04 10:41:22 ocp-sno3 ostree[5337]: Completed syncfs() for system root in 1 ms
Feb 04 10:41:22 ocp-sno3 ostree[5337]: Starting freeze/thaw cycle for system root
Feb 04 10:41:22 ocp-sno3 ostree[5337]: Completed freeze/thaw cycle for system root in 31 ms
Feb 04 10:41:22 ocp-sno3 ostree[5337]: Bootloader updated; bootconfig swap: yes; bootversion: boot.1.1, deployment count change: -1
...
Broadcast message from root@localhost (Tue 2025-02-04 10:41:25 UTC):
...
The system will power off now!
-----

[#creating-the-configuration-iso]
== Creating the Configuration ISO

Now that the host is powered off, it is time to complete the installation by reconfiguring the cluster. The Image Based Install (IBI) Operator running in the hub cluster is responsible for creating the configuration ISO and boot the host with it attached. 

Let's verify IBIO is installed and running in the multicluster-engine namespace of the hub cluster:

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/hub-kubeconfig get pods -n multicluster-engine -lapp=image-based-install-operator
-----
[.console]
[source,bash,subs="attributes+,+macros"]
-----
NAME                                            READY   STATUS    RESTARTS   AGE
image-based-install-operator-7f7659f86c-cd46k   2/2     Running   0          5h
-----

Once we verified the operator is running in our hub cluster we can start creating the site-specific configuration resources. First, create the target cluster namespace:

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
cat <<EOF > ~/5g-deployment-lab/ztp-repository/site-configs/pre-reqs/sno3/ns.yaml
---
apiVersion: v1
kind: Namespace
metadata:
  name: sno3
EOF
-----

Next, the authentication credentials will be needed to pull the seed container image from the registry:

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
cat <<EOF > ~/5g-deployment-lab/ztp-repository/site-configs/pre-reqs/sno3/pull-secret.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: disconnected-registry-pull-secret
  namespace: sno3
stringData:
  .dockerconfigjson: '{"auths":{"infra.5g-deployment.lab:8443":{"auth":"YWRtaW46cjNkaDR0MSE="}}}'
  type: kubernetes.io/dockerconfigjson
EOF
-----

The credentials to access the BMC are required in order to mount the configuration ISO and also booting the node automatically:

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
cat <<EOF > ~/5g-deployment-lab/ztp-repository/site-configs/pre-reqs/sno3/bmc-credentials.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: sno3-bmc-credentials
  namespace: sno3
data:
  username: "YWRtaW4="
  password: "YWRtaW4="
type: Opaque
EOF
-----

Next, we have to create a `ConfigMap` resource to define additional manifests in our image-based deployment. Based on our seed image we have to define:

* The disconnected catalog source that we will allow us to install additional operators in the target SNO cluster.
* The LVMCluster configuration for the LVM Operator. Note that the LVM Operator was included in the seed image.
* The ImageDigestMirrorSet required in our disconnected environment.

IMPORTANT: Remember that during the seed image generation, the catalogsources are removed from the seed image. Also, the LVMCluster CR is not allowed to exist in the seed cluster, it has to be added within the reconfiguration process. More information on the prerequisites for generating a seed image {seed-prereq-config}[here].

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
cat <<EOF > ~/5g-deployment-lab/ztp-repository/site-configs/pre-reqs/sno3/ibi_extra_manifests.yaml
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: disconnected-ran-config
  namespace: sno3
data:
  redhat-operator-index.yaml: |
    apiVersion: operators.coreos.com/v1alpha1
    kind: CatalogSource
    metadata:
      annotations:
        target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
      name: redhat-operator-index
      namespace: openshift-marketplace
    spec:
      displayName: default-cat-source
      image: infra.5g-deployment.lab:8443/prega/prega-operator-index:v4.18
      publisher: Red Hat
      sourceType: grpc
      updateStrategy:
        registryPoll:
          interval: 1h
  lvmcluster.yaml: |
    apiVersion: lvm.topolvm.io/v1alpha1
    kind: LVMCluster
    metadata:
      name: lvmcluster
      namespace: openshift-storage
    spec:
      storage:
        deviceClasses:
        - deviceSelector:
            paths:
            - /dev/vdb
          fstype: xfs
          name: vg1
          thinPoolConfig:
            chunkSizeCalculationPolicy: Static
            metadataSizeCalculationPolicy: Host
            name: thin-pool-1
            overprovisionRatio: 10
            sizePercent: 90
  ImageDigestSources.yaml: |
    apiVersion: config.openshift.io/v1
    kind: ImageDigestMirrorSet
    metadata:
      name: image-digest-mirror
    spec:
      imageDigestMirrors:
      - mirrors:
        - infra.5g-deployment.lab:8443/prega/test/openshift4
        source: registry.redhat.io/openshift4
      - mirrors:
        - infra.5g-deployment.lab:8443/prega/test/rhacm2
        source: registry.redhat.io/rhacm2
      - mirrors:
        - infra.5g-deployment.lab:8443/prega/test/rhceph
        source: registry.redhat.io/rhceph
      - mirrors:
        - infra.5g-deployment.lab:8443/prega/test/rhel8
        source: registry.redhat.io/rhel8
      - mirrors:
        - infra.5g-deployment.lab:8443/prega/test/rh-sso-7
        source: registry.redhat.io/rh-sso-7
      - mirrors:
        - infra.5g-deployment.lab:8443/prega/test/odf4
        source: registry.redhat.io/odf4
      - mirrors:
        - infra.5g-deployment.lab:8443/prega/test/multicluster-engine
        source: registry.redhat.io/multicluster-engine
      - mirrors:
        - infra.5g-deployment.lab:8443/prega/test/openshift-gitops-1
        source: registry.redhat.io/openshift-gitops-1
      - mirrors:
        - infra.5g-deployment.lab:8443/prega/test/lvms4
        source: registry.redhat.io/lvms4
      - mirrors:
        - infra.5g-deployment.lab:8443/prega/test/rh-osbs
        source: quay.io/prega/test/rh-osbs
      - mirrors:
        - infra.5g-deployment.lab:8443/prega/test/oadp
        source: registry.redhat.io/oadp
      - mirrors:
        - infra.5g-deployment.lab:8443/openshift-release-dev/ocp-v4.0-art-dev
        source: quay.io/openshift-release-dev/ocp-v4.0-art-dev
      - mirrors:
        - infra.5g-deployment.lab:8443/openshift-release-dev/ocp-release
        source: quay.io/openshift-release-dev/ocp-release
    status: {}
EOF
-----

Finally, add the Kustomization files so the previous manifests can be applied using a ZTP GitOps worflow.

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
cat <<EOF > ~/5g-deployment-lab/ztp-repository/site-configs/pre-reqs/sno3/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - bmc-credentials.yaml
  - pull-secret.yaml
  - ns.yaml
  - ibi_extra_manifests.yaml
EOF
-----

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
cat <<EOF > ~/5g-deployment-lab/ztp-repository/site-configs/pre-reqs/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - sno3/
EOF
-----

Eventually, let's create the ClusterInstance CR, which is an abstraction layer on top of the different components managed by the SiteConfig Operator used to deploy an OpenShift cluster. Notice that in this installation we are using the Image Based Installation flow. We can see that by checking the templateRefs specifications: `ibi-node-templates-v1` and `ibi-cluster-templates-v1` which are the default ones for IBI.

IMPORTANT: The SiteConfig Operator transforms the ClusterInstance CR into multiple Installation Manifests as detailed in link:ztp-workflow.html#ibi-deployment[Image Based Installation] section. With some of those manifests that define site-specific resources, the IBI operator creates a configuration ISO to start the reconfiguration process.

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
cat << 'EOF' > ~/5g-deployment-lab/ztp-repository/site-configs/hub-1/ocp-sno3.yaml
---
apiVersion: siteconfig.open-cluster-management.io/v1alpha1
kind: ClusterInstance
metadata:
  name: "ocp-sno3"
  namespace: "sno3"
spec:
  additionalNTPSources:
    - "clock.corp.redhat.com"
  baseDomain: "5g-deployment.lab"
  clusterImageSetNameRef: "active-ocp-version"
  clusterName: "sno3"
  clusterNetwork:
    - cidr: "10.128.0.0/14"
      hostPrefix: 23
  cpuPartitioningMode: AllNodes
  extraLabels:
    ManagedCluster:
      common: "ocp418"
      logicalGroup: "active"
      group-du-sno: ""
      du-site: "sno3"
      hardware-type: "hw-type-platform-1"
  holdInstallation: false
  extraManifestsRefs:
    - name: disconnected-ran-config
  machineNetwork:
    - cidr: "192.168.125.0/24"
  networkType: "OVNKubernetes"
  pullSecretRef:
    name: "disconnected-registry-pull-secret"
  serviceNetwork:
    - cidr: "172.30.0.0/16"
  sshPublicKey: "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAACAQC5pFKFLOuxrd9Q/TRu9sRtwGg2PV+kl2MHzBIGUhCcR0LuBJk62XG9tQWPQYTQ3ZUBKb6pRTqPXg+cDu5FmcpTwAKzqgUb6ArnjECxLJzJvWieBJ7k45QzhlZPeiN2Omik5bo7uM/P1YIo5pTUdVk5wJjaMOb7Xkcmbjc7r22xY54cce2Wb7B1QDtLWJkq++eJHSX2GlEjfxSlEvQzTN7m2N5pmoZtaXpLKcbOqtuSQSVKC4XPgb57hgEs/ZZy/LbGGHZyLAW5Tqfk1JCTFGm6Q+oOd3wAOF1SdUxM7frdrN3UOB12u/E6YuAx3fDvoNZvcrCYEpjkfrsjU91oz78aETZV43hOK9NWCOhdX5djA7G35/EMn1ifanVoHG34GwNuzMdkb7KdYQUztvsXIC792E2XzWfginFZha6kORngokZ2DwrzFj3wgvmVyNXyEOqhwi6LmlsYdKxEvUtiYhdISvh2Y9GPrFcJ5DanXe7NVAKXe5CyERjBnxWktqAPBzXJa36FKIlkeVF5G+NWgufC6ZWkDCD98VZDiPP9sSgqZF8bSR4l4/vxxAW4knKIZv11VX77Sa1qZOR9Ml12t5pNGT7wDlSOiDqr5EWsEexga/2s/t9itvfzhcWKt+k66jd8tdws2dw6+8JYJeiBbU63HBjxCX+vCVZASrNBjiXhFw=="
  templateRefs:
    - name: ibi-cluster-templates-v1
      namespace: open-cluster-management
  nodes:
    - automatedCleaningMode: "disabled"
      bmcAddress: "redfish-virtualmedia://192.168.125.1:9000/redfish/v1/Systems/local/sno3"
      bmcCredentialsName:
        name: "sno3-bmc-credentials"
      bootMACAddress: "AA:AA:AA:AA:04:01"
      bootMode: "UEFI"
      hostName: "ocp-sno3.sno3.5g-deployment.lab"
      nodeNetwork:
        interfaces:
          - name: "enp3s0"
            macAddress: "AA:AA:AA:AA:04:01"
        config:
          interfaces:
            - name: enp3s0
              type: ethernet
              state: up
              ipv4:
                enabled: true
                dhcp: true
              ipv6:
                enabled: false
      role: "master"
      rootDeviceHints:
        deviceName: "/dev/vda"
      templateRefs:
        - name: ibi-node-templates-v1
          namespace: open-cluster-management
EOF
-----

Add the Kustomization file to pull the ClusterInstance recently created:

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
cat << 'EOF' > ~/5g-deployment-lab/ztp-repository/site-configs/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - pre-reqs/
  - resources/
  - hub-1/ocp-sno3.yaml
EOF
-----

Lastly, commit all these changes to our link:lab-environment-introduction.html#git-server[Git Server] repository.

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
cd ~/5g-deployment-lab/ztp-repository/site-configs/
git add --all
git commit -m 'Added SNO3 cluster using image-based install flow'
git push origin main
cd ~
-----

[#deploying-sno-using-gitops-pipeline]
== Deploying the SNO Cluster using the ZTP GitOps Pipeline

When we applied the ZTP GitOps Pipeline configuration in the last section, that created two ArgoCD apps. One of the apps (clusters) take care of deploying the clusters defined in the `ClusterInstance` while the other (policies) take care of deploying the policies defined in the different `PolicyGenTemplates`.

Since the apps have been created, ArgoCD started doing its magic which means that the cluster deployment should already be running, let's see what happened in Argo CD.

1. Login into https://openshift-gitops-server-openshift-gitops.apps.hub.5g-deployment.lab/[Argo CD].
2. When accessing choose `Logging with OpenShift`, on the next screen use the OpenShift Console Admin credentials.
3. You will see the follow applications:
+
image::argocd-apps.png[ArgoCD Apps]
+
4. From all these apps, the ones related to the ZTP GitOps Pipeline are `clusters` and `policies`. If we click on `clusters` we will see the following screen:
+
image::argocd-clusters-app.png[ArgoCD Clusters App]
+
5. You can see how the pipeline created all the required objects to get our site deployed.
6. If we check the `policies` app this is what we will see:
+
image::argocd-policies-app.png[ArgoCD Policies App]


[#completing-install]
== Completing the Installation

Once the changes were commited and pushed to the Git repository, the link:ocp-gitops.html[OpenShift GitOps] operator is responsible for reconciling them. Next, we can monitor the status of the reconfiguration process. We can use the following command to see the progress from the hub cluster. Notice that a `dataimage` object has been created. This object contains information about the configuration ISO created in the previous section link:installing-imagebaseinstall-cluster.html#creating-the-configuration-iso[Creating the Configuration ISO].

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/hub-kubeconfig -n sno3 get bmh,clusterinstance,clusterdeployment,imageclusterinstall,dataimage
-----
[console-input]
[source,bash,subs="attributes+,+macros"]
-----
NAMESPACE   NAME                               STATE                    CONSUMER   ONLINE   ERROR   AGE
sno3        baremetalhost.metal3.io/ocp-sno3   externally provisioned              true             35s

NAME                                                             PAUSED   PROVISIONSTATUS   PROVISIONDETAILS         AGE
clusterinstance.siteconfig.open-cluster-management.io/ocp-sno3            Completed         Provisioning completed   9h

NAMESPACE   NAME                                       INFRAID                                PLATFORM          REGION   VERSION       CLUSTERTYPE   PROVISIONSTATUS   POWERSTATE   AGE
sno3        clusterdeployment.hive.openshift.io/sno3   sno3-zrts7                             none-platform                                          Provisioning                   35s

NAMESPACE   NAME                                                    REQUIREMENTSMET           COMPLETED                       BAREMETALHOSTREF
sno3        imageclusterinstall.extensions.hive.openshift.io/sno3   HostValidationSucceeded   ClusterInstallationInProgress   ocp-sno3

NAMESPACE   NAME                           AGE
sno3        dataimage.metal3.io/ocp-sno3   34s
-----

From the Red Hat ACM console in the hub cluster we can also see the progress.

image::ibi-acm-reconfig01.png[IBI Installation in ACM]

Let's see how the OpenShift installation is progressing in the target cluster. First, we need the sno3 credentials. From the hub cluster we can obtain them using this command:
[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/hub-kubeconfig -n sno3 extract secret/sno3-admin-kubeconfig --to=- > ~/sno3-kubeconfig
-----

Now, that we have the sno3 admin credentials, let's see the current status of the cluster and how the different cluster operators are moving forward:

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
watch oc --kubeconfig ~/sno3-kubeconfig get clusterversion,nodes,mcp,co
-----
[.console]
[source,bash,subs="attributes+,+macros"]
-----
NAME                                         VERSION	   AVAILABLE   PROGRESSING   SINCE   STATUS
clusterversion.config.openshift.io/version   {sno-cluster-version1-cvo}   False       False         4h54m   Error while reconciling {sno-cluster-version1-cvo}: an unknown error has occurred: MultipleErrors

NAME                                   STATUS   ROLES                         AGE     VERSION
node/ocp-sno3.sno3.5g-deployment.lab   Ready    control-plane,master,worker   6m40s   v1.31.4

NAME                                                         CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
machineconfigpool.machineconfiguration.openshift.io/master   rendered-master-484df4de53ef04b47b119185a6025d18   True	  False      False	1              1                   1                     0                      5h14m
machineconfigpool.machineconfiguration.openshift.io/worker   rendered-worker-ca41ad0a46d9e710d6540dafbcbbb473   True	  False      False	0              0                   0                     0                      5h14m

NAME                                                                           VERSION       AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
clusteroperator.config.openshift.io/authentication                             {sno-cluster-version1-cvo}   False	 False         True	  5m39s   OAuthServerRouteEndpointAccessibleControllerAvailable: Get "https://oauth-openshift.apps.sno3.5g-dep
loyment.lab/healthz": EOF
clusteroperator.config.openshift.io/config-operator                            {sno-cluster-version1-cvo}   True        False         False	  5h14m
clusteroperator.config.openshift.io/dns                                        {sno-cluster-version1-cvo}   True        False         False	  5m25s
clusteroperator.config.openshift.io/etcd                                       {sno-cluster-version1-cvo}   True        False         False	  5h5m
clusteroperator.config.openshift.io/ingress                                    {sno-cluster-version1-cvo}   True        False         True	  5m38s   The "default" ingress controller reports Degraded=True: DegradedConditions: One or more other status
 conditions indicate a degraded state: CanaryChecksSucceeding=Unknown (CanaryRouteNotAdmitted: Canary route is not admitted by the default ingress controller)
clusteroperator.config.openshift.io/kube-apiserver                             {sno-cluster-version1-cvo}   True        False         False	  4h54m
clusteroperator.config.openshift.io/kube-controller-manager                    {sno-cluster-version1-cvo}   True        False         False	  4h57m
clusteroperator.config.openshift.io/kube-scheduler                             {sno-cluster-version1-cvo}   True        False         False	  5h2m
clusteroperator.config.openshift.io/kube-storage-version-migrator              {sno-cluster-version1-cvo}   True        False         False	  5h14m
clusteroperator.config.openshift.io/machine-approver                           {sno-cluster-version1-cvo}   True        False         False	  5h14m
clusteroperator.config.openshift.io/machine-config                             {sno-cluster-version1-cvo}   True        False         False	  5h14m
clusteroperator.config.openshift.io/monitoring                                 {sno-cluster-version1-cvo}   True        False         False	  4h57m
clusteroperator.config.openshift.io/network                                    {sno-cluster-version1-cvo}   True        False         False	  5h14m
clusteroperator.config.openshift.io/node-tuning                                {sno-cluster-version1-cvo}   True        False         False	  5m38s
clusteroperator.config.openshift.io/openshift-apiserver                        {sno-cluster-version1-cvo}   True        False         False	  5m32s
clusteroperator.config.openshift.io/openshift-controller-manager               {sno-cluster-version1-cvo}   True        False         False	  5m22s
clusteroperator.config.openshift.io/operator-lifecycle-manager                 {sno-cluster-version1-cvo}   True        False         False	  5h14m
clusteroperator.config.openshift.io/operator-lifecycle-manager-catalog         {sno-cluster-version1-cvo}   True        False         False	  5h14m
clusteroperator.config.openshift.io/operator-lifecycle-manager-packageserver   {sno-cluster-version1-cvo}   True        False         False	  5m31s
clusteroperator.config.openshift.io/service-ca                                 {sno-cluster-version1-cvo}   True        False         False	  5h14m
-----

The installation process takes less than *10 minutes*. You can double check that the reconfiguration succeeded by running the following command from the hub cluster.

WARNING: Confirm that the `clusterInstance` is shown as "Provisioning completed", the `imageclusterinstall` status is ClusterInstallationSucceeded and the `clusterdeployment` is Provisioned.

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/hub-kubeconfig -n sno3 get bmh,clusterinstance,clusterdeployment,imageclusterinstall,dataimage
-----
[console-input]
[source,bash,subs="attributes+,+macros"]
-----
NAME                               STATE                    CONSUMER   ONLINE   ERROR   AGE
baremetalhost.metal3.io/ocp-sno3   externally provisioned              true             32m

NAME                                                             PAUSED   PROVISIONSTATUS   PROVISIONDETAILS         AGE
clusterinstance.siteconfig.open-cluster-management.io/ocp-sno3            Completed         Provisioning completed   32m

NAME                                       INFRAID	PLATFORM        REGION   VERSION       CLUSTERTYPE   PROVISIONSTATUS   POWERSTATE   AGE
clusterdeployment.hive.openshift.io/sno3   sno3-vcd52   none-platform            {sno-cluster-version1-cvo}                 Provisioned       Running      32m

NAME                                                    REQUIREMENTSMET           COMPLETED                      BAREMETALHOSTREF
imageclusterinstall.extensions.hive.openshift.io/sno3   HostValidationSucceeded   ClusterInstallationSucceeded   ocp-sno3

NAME                           AGE
dataimage.metal3.io/ocp-sno3   32m
-----

Notice that the Red Hat ACM console will show the sno3 OpenShift cluster as Ready:

image::ibi-acm-reconfig02.png[IBI Installation completed]


The Add-ons applied via KlusterletAddonConfig CR are also installed and available:

image::ibi-acm-reconfig03.png[IBI Add-ons installed]

[#verifying-install]
== Verifying the RAN DU Configuration

As it was explained in link:ztp-workflow.html[ZTP Workflow] section, the workflow ends with the CNF workload deployed and running on the site nodes. In our case, the workload is not yet deployed but the ground is set for them to be executed. That means the {rds-config}[Telco RAN DU reference configuration] is applied.

First, confirm that OpenShift,e.g. the platform, is installed with the version {sno-cluster-version1-cvo}:

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/sno3-kubeconfig get clusterversion,nodes
-----
[.console]
[source,bash,subs="attributes+,+macros"]
-----
NAME                                         VERSION       AVAILABLE   PROGRESSING   SINCE   STATUS
clusterversion.config.openshift.io/version   {sno-cluster-version1-cvo}   True        False         6h4m    Cluster version is {sno-cluster-version1-cvo}

NAME                                   STATUS   ROLES                         AGE   VERSION
node/ocp-sno3.sno3.5g-deployment.lab   Ready    control-plane,master,worker   32m   v1.31.4
-----

Check the operators installed. Note that all these operators were included in the seed image:

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/sno3-kubeconfig get operators
-----
[.console]
[source,bash,subs="attributes+,+macros"]
-----
NAME                                                      AGE
lifecycle-agent.openshift-lifecycle-agent                 6h5m
lvms-operator.openshift-storage                           6h5m
redhat-oadp-operator.openshift-adp                        6h5m
sriov-network-operator.openshift-sriov-network-operator   6h5m
-----

Check that the LVMS Operator is ready to provide the cluster with persistent storage. Remember that we added an link:installing-imagebaseinstall-cluster.html#creating-the-configuration-iso[extra-manifest] in the form of a configMap to our image-based installation to configure it.

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/sno3-kubeconfig get sc,lvmcluster -A
-----
[.console]
[source,bash,subs="attributes+,+macros"]
-----
NAME                                   PROVISIONER   RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
storageclass.storage.k8s.io/lvms-vg1   topolvm.io    Delete          WaitForFirstConsumer   true                   32m

NAMESPACE           NAME                                   STATUS
openshift-storage   lvmcluster.lvm.topolvm.io/lvmcluster   Ready
-----

Check that the SR-IOV interfaces are ready to be used by our CNFs and the performance profile applied:

WARNING: The SR-IOV configuration was included in the seed cluster because the hardware of both the seed and target cluster was exactly the same. In case the SR-IOV configuration differs, it can be included as another extra-manifest the same way the LMV operator was reconfigured. More information can be found {seed-extramanifests}[here].

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/sno3-kubeconfig describe node | grep Allocatable: -A10
-----
[.console]
[source,bash,subs="attributes+,+macros"]
-----
Allocatable:
  cpu:                                     8
  ephemeral-storage:                       56362847342
  hugepages-1Gi:                           4Gi
  hugepages-2Mi:                           0
  management.workload.openshift.io/cores:  12k
  memory:                                  13530204Ki
  openshift.io/virt-enp4s0:                2
  openshift.io/virt-enp5s0:                2
  pods:                                    250
-----

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/sno3-kubeconfig get performanceprofile
-----
[.console]
[source,bash,subs="attributes+,+macros"]
-----
NAME                                 AGE
openshift-node-performance-profile   6h24m
-----

At this stage the sno3 is deployed and configured with the RAN DU profile, ready to run telco CNF applications.