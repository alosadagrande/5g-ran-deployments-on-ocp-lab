= Using TALM to Update Clusters
include::_attributes.adoc[]
:profile: 5g-ran-lab

In this section, we will perform a platform upgrade to both managed clusters using the pre-cache and backup feature implemented in the Topology Aware Lifecycle Manager (TALM) operator. The pre-cache feature prepares the maintenance operation in the managed clusters by pulling the required artifacts prior to the upgrade. The reasoning behind this feature is that SNO spoke clusters may have limited bandwidth to the container registry, which will make it difficult for the upgrade to complete within the required time. In order to ensure the upgrade can fit within the maintenance window, the required artifacts need to be present on the spoke cluster prior to the upgrade. The idea is pre-caching all the images needed for the platform and operator upgrade on the node, so they are not pulled at upgrade time. Do it in a maintenance window(s) before the upgrade maintenance window.

The backup feature, on the other hand, implements a procedure for rapid recovery of a SNO in the event of a failed upgrade that is unrecoverable. The SNO needs to be restored to a working state with the previous version of OCP without requiring a re-provision of the application(s).

IMPORTANT: The backup feature only allows SNOs to be restored, this is not applicable to any other kind of OpenShift clusters.

Let's upgrade our both clusters. First of all, let's verify the TALM operator is running in our **hub cluster**:

[#verify-talm]
== Verifying the TALM state

IMPORTANT: Below commands must be executed from the workstation host if not specified otherwise.

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/5g-deployment-lab/hub-kubeconfig get operators
-----

[console-input]
[source,console]
-----
NAME                                                   AGE
advanced-cluster-management.open-cluster-management    103m
ansible-automation-platform-operator.aap               103m
lvms-operator.openshift-storage                        105m
multicluster-engine.multicluster-engine                103m
openshift-gitops-operator.openshift-operators          105m
topology-aware-lifecycle-manager.openshift-operators   103m
-----

Next, double check there is no problem with the Pod. Notice that the name of the Pod is cluster-group-upgrade-controller-manager, based on the name of the upstream project {talm-upstream-project}[Cluster Group Upgrade Operator].


[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/5g-deployment-lab/hub-kubeconfig get pods,sa,deployments -n openshift-operators
-----

[console-input]
[source,console]
-----
NAME                                                                READY   STATUS    RESTARTS   AGE
pod/cluster-group-upgrades-controller-manager-v2-789fd8fbcd-nn4k5   2/2     Running   0          103m
pod/openshift-gitops-operator-controller-manager-6794f4f9cc-vpm2b   2/2     Running   0          106m

NAME                                                                SECRETS   AGE
serviceaccount/builder                                              1         127m
serviceaccount/cluster-group-upgrades-controller-manager            1         103m
serviceaccount/cluster-group-upgrades-operator-controller-manager   1         103m
serviceaccount/default                                              1         132m
serviceaccount/deployer                                             1         127m
serviceaccount/openshift-gitops-operator-controller-manager         1         106m

NAME                                                           READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/cluster-group-upgrades-controller-manager-v2   1/1     1            1           103m
deployment.apps/openshift-gitops-operator-controller-manager   1/1     1            1           106m
-----

Finally, let's take a look at the cluster group upgrade (CGU) CRD managed by TALM. If we pay a closer look we will notice that an already completed CGU was applied to SNO2. As we mentioned in link:talm.html#inform-policies[inform policies] section, all policies are not enforced, the user has to create the proper CGU resource to enforce them. However, when using ZTP, we want our cluster provisioned and configured automatically. This is where TALM will step through the set of created policies (inform) and will enforce them once the cluster was successfully provisioned. Therefore, the configuration stage starts without any intervention ending up with our OpenShift cluster ready to process workloads.

WARNING: It's possible that you get `UpgradeNotCompleted`, if that's the case you need to wait for the remaining policies to be applied. You can check policies status https://console-openshift-console.apps.hub.5g-deployment.lab/multicloud/governance/policies[here].

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/5g-deployment-lab/hub-kubeconfig get cgu sno2 -n ztp-install
-----

[console-input]
[source,console]
-----
NAME   AGE     STATE       DETAILS
sno2   7m26s   Completed   All clusters are compliant with all the managed policies
-----

[#getting-snos-kubeconfigs]
== Getting the SNO clusters kubeconfigs

In the previous sections we have deployed the `sno2` cluster and attached the `sno1` cluster. Before we continue with TALM, let's grab the kubeconfigs for both cluster since we will need them for the next sections.

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/5g-deployment-lab/hub-kubeconfig -n sno2 extract secret/sno2-admin-kubeconfig --to=- > ~/5g-deployment-lab/sno2-kubeconfig
oc --kubeconfig ~/5g-deployment-lab/hub-kubeconfig -n sno3 extract secret/sno3-admin-kubeconfig --to=- > ~/5g-deployment-lab/sno3-kubeconfig
-----

[#install-lca]
== Install the LifeCycle Agent Operator

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
cat <<EOF > ~/5g-deployment-lab/ztp-repository/site-policies/fleet/active/requirements-upgrade.yaml
---
apiVersion: ran.openshift.io/v1
kind: PolicyGenTemplate
metadata:
  name: "requirements-upgrade"
  namespace: "ztp-policies"
spec:
  bindingRules:
    common: "ocp418"
    logicalGroup: "active"
    du-zone: europe
  mcp: master
  remediationAction: inform
  sourceFiles:
    - fileName: LcaSubscriptionOperGroup.yaml
      metadata:
        name: lifecycle-agent-operatorgroup
      policyName: subscriptions-policy
    - fileName: LcaSubscription.yaml
      spec:
        channel: stable
        source: redhat-operator-index
      policyName: subscriptions-policy
    - fileName: LcaOperatorStatus.yaml
      policyName: subscriptions-policy
    - fileName: LcaSubscriptionNS.yaml
      policyName: subscriptions-policy
    - fileName: OadpSubscriptionOperGroup.yaml
      policyName: subscriptions-policy
    - fileName: OadpSubscription.yaml
      spec:
        source: redhat-operator-index
      policyName: subscriptions-policy
    - fileName: OadpOperatorStatus.yaml
      policyName: subscriptions-policy
    - fileName: OadpSubscriptionNS.yaml
      policyName: subscriptions-policy

    - fileName: OadpSecret.yaml
      data:
        cloud: W2RlZmF1bHRdCmF3c19hY2Nlc3Nfa2V5X2lkPWFkbWluCmF3c19zZWNyZXRfYWNjZXNzX2tleT1hZG1pbjEyMzQK 
      policyName: config-policy
    - fileName: OadpDataProtectionApplication.yaml
      spec:
        backupLocations:
        - velero:
            config:
              insecureSkipTLSVerify: "true"
              profile: default
              region: minio
              s3ForcePathStyle: "true"
              s3Url: http://192.168.125.1:9002
            credential:
              key: cloud
              name: cloud-credentials
            default: true
            objectStorage:
              bucket: klusterlet-backup
              prefix: velero
            provider: aws
      policyName: config-policy
    - fileName: OadpBackupStorageLocationStatus.yaml
      policyName: config-policy
    - fileName: ConfigMapGeneric.yaml
      complianceType: mustonlyhave
      policyName: "oadp-cm-policy"
      metadata:
        name: oadp-lvms
        namespace: openshift-adp
      data:
        PlatformBackupRestoreLvms.yaml: |
          ---
          apiVersion: velero.io/v1
          kind: Backup
          metadata:
            name: lvmcluster
            namespace: openshift-adp
            labels:
              velero.io/storage-location: default
          spec:
            includedNamespaces:
            - openshift-storage
            includedNamespaceScopedResources:
            - lvmclusters
            - lvmvolumegroups
            - lvmvolumegroupnodestatuses
          ---
          apiVersion: velero.io/v1
          kind: Restore
          metadata:
            name: lvmcluster
            namespace: openshift-adp
            labels:
              velero.io/storage-location: default
            annotations:
              lca.openshift.io/apply-wave: "2"
          spec:
            backupName:
              lvmcluster
    - fileName: ConfigMapGeneric.yaml
      complianceType: mustonlyhave
      policyName: "oadp-cm-policy"
      metadata:
        name: oadp-lvms
        namespace: openshift-adp
      data:
        PlatformBackupRestoreWithIBGU.yaml: |
           
       
#    - fileName: PlatformBackupRestoreWithIBGU.yaml
#      metadata:
#        annotations:
#          ran.openshift.io/ztp-deploy-wave: "100"
#      policyName: config-policy
#    - fileName: PlatformBackupRestoreLvms.yaml
#      metadata:
#        annotations:
#          ran.openshift.io/ztp-deploy-wave: "100"
#      policyName: config-policy
EOF
-----

Modify the kustomization.yaml inside the site-policies folder, so it includes this new PGT and eventually will be applied by ArgoCD.

NOTE: If you're using MacOS and you're getting errors while running `sed -i` commands, make sure you are using `gsed`. If you do not have it available, please install it: `brew install gnu-sed`.

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
sed -i "/- group-du-sno.yaml/a \ \ - requirements-upgrade.yaml" ~/5g-deployment-lab/ztp-repository/site-policies/fleet/active/kustomization.yaml
-----

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
cat <<EOF > ~/5g-deployment-lab/ztp-repository/site-policies/fleet/active/kustomization.yaml
---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
generators:
  - common-418.yaml
  - group-du-sno.yaml
  - requirements-upgrade.yaml
#  - group-du-sno-validator.yaml

configMapGenerator:
- files:
  - source-crs/reference-crs/ibu/PlatformBackupRestoreWithIBGU.yaml
  - source-crs/reference-crs/ibu/PlatformBackupRestoreLvms.yaml
  name: oadp-cm
  namespace: ztp-policies
generatorOptions:
  disableNameSuffixHash: true

resources:
  - group-hardware-types-configmap.yaml
EOF
-----


Then commit all the changes:

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
cd ~/5g-deployment-lab/ztp-repository/
git add site-policies/fleet/active/requirements-upgrade.yaml site-policies/fleet/active/kustomization.yaml
git commit -m "adds upgrade policy"
git push origin main 
-----

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/5g-deployment-lab/hub-kubeconfig label managedcluster sno3 du-zone=europe logicalGroup=active
-----


Enforce the policies

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
cat <<EOF | oc --kubeconfig ~/5g-deployment-lab/hub-kubeconfig apply -f -
---
apiVersion: ran.openshift.io/v1alpha1
kind: ClusterGroupUpgrade
metadata:
  name: requirements-upgrade
  namespace: ztp-policies
spec:
  clusters:
  - sno2
  - sno3
  managedPolicies:
  - requirements-upgrade-subscriptions-policy
  - requirements-upgrade-config-policy
  remediationStrategy:
    maxConcurrency: 2
    timeout: 240
EOF
-----

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
$ watch oc get cgu -A
                                                                                                                                                            localhost.localdomain: 
NAMESPACE      NAME                   AGE   STATE        DETAILS
ztp-install    local-cluster          19h   Completed    All clusters already compliant with the specified managed policies
ztp-install    sno2                   13m   Completed    All clusters are compliant with all the managed policies
ztp-install    sno3                   11h   Completed    All clusters already compliant with the specified managed policies
ztp-policies   requirements-upgrade   35s   InProgress   Remediating non-compliant policies
-----

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/5g-deployment-lab/hub-kubeconfig get cgu -n ztp-policies
NAME                   AGE   STATE       DETAILS
requirements-upgrade   19m   Completed   All clusters are compliant with all the managed policies
-----


[#upgrade-policy-creation]
== Creating the Image Base Group Upgrade 


[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
cat <<EOF | oc --kubeconfig ~/5g-deployment-lab/hub-kubeconfig apply -f -
apiVersion: v1
kind: ConfigMap
metadata:
  name: catalog-source-disconnected
  namespace: openshift-lifecycle-agent
data:
  redhat-operator-index.yaml: |
    apiVersion: operators.coreos.com/v1alpha1
    kind: CatalogSource
    metadata:
      annotations:
        target.workload.openshift.io/management: '{"effect": "PreferredDuringScheduling"}'
      name: redhat-operator-index
      namespace: openshift-marketplace
    spec:
      displayName: default-cat-source
      image: infra.5g-deployment.lab:8443/prega/prega-operator-index:v4.18
      publisher: Red Hat
      sourceType: grpc
      updateStrategy:
        registryPoll:
          interval: 1h
EOF
-----




[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
cat <<EOF | oc --kubeconfig ~/5g-deployment-lab/hub-kubeconfig apply -f -
apiVersion: lcm.openshift.io/v1alpha1
kind: ImageBasedGroupUpgrade
metadata:
  name: telco5g-lab
  namespace: default
spec:
  clusterLabelSelectors: 
    - matchExpressions:
      - key: name
        operator: In
        values:
        - sno2
        - sno3
  ibuSpec:
    seedImageRef: 
      image: infra.5g-deployment.lab:8443/ibi/lab5gran:v4.18.0-rc.8
      version: 4.18.0-rc.8
 #   extraManifests: 
 #     - name: catalog-source-disconnected
 #       namespace: openshift-lifecycle-agent  
  oadpContent: 
    - name: oadp-cm-example
      namespace: openshift-adp


  plan: 
    - actions: ["Prep", "Upgrade", "FinalizeUpgrade"]
      rolloutStrategy:
        maxConcurrency: 10
        timeout: 2400
EOF
-----


[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
Every 2.0s: oc get ibgu,cgu -A                                                                                                                                                                             katatonic: Thu Feb 13 10:23:40 2025

NAMESPACE   NAME                                                  AGE
default     imagebasedgroupupgrade.lcm.openshift.io/telco5g-lab   102s

NAMESPACE      NAME                                                                              AGE    STATE        DETAILS
default        clustergroupupgrade.ran.openshift.io/telco5g-lab-prep-upgrade-finalizeupgrade-0   102s   InProgress   Rolling out manifestworks
ztp-install    clustergroupupgrade.ran.openshift.io/local-cluster                                20h    Completed    All clusters already compliant with the specified managed policies
ztp-install    clustergroupupgrade.ran.openshift.io/sno2                                         47m    Completed    All clusters are compliant with all the managed policies
ztp-install    clustergroupupgrade.ran.openshift.io/sno3                                         12h    Completed    All clusters already compliant with the specified managed policies
ztp-policies   clustergroupupgrade.ran.openshift.io/requirements-upgrade                         34m    Completed    All clusters are compliant with all the managed policies
-----

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc get ibgu -n default telco5g-lab -ojson | jq .status                                                                                                                                                        
{
  "clusters": [
    {
      "currentAction": {
        "action": "Prep"
      },
      "name": "sno2"
    },
    {
      "currentAction": {
        "action": "Prep"
      },
      "name": "sno3"
    }
  ],
  "conditions": [
    {
      "lastTransitionTime": "2025-02-13T09:22:00Z",
      "message": "Waiting for plan step 0 to be completed",
      "reason": "InProgress",
      "status": "True",
      "type": "Progressing"
    }
  ],
  "observedGeneration": 1
-----

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/5g-deployment-lab/sno2-kubeconfig  get ibu -A
NAME      AGE   DESIRED STAGE   STATE        DETAILS
upgrade   36s   Prep            InProgress   Stateroot setup job in progress. job-name: lca-prep-stateroot-setup, job-namespace: openshift-lifecycle-agent
-----

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/5g-deployment-lab/sno2-kubeconfig  get ibu -A
NAME      AGE    DESIRED STAGE   STATE        DETAILS
upgrade   118s   Prep            InProgress   Precache job in progress. job-name: lca-prep-precache, job-namespace: openshift-lifecycle-agent. total: 83 (pulled: 23, failed: 1)
-----

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc get ibgu -n default telco5g-lab -ojson | jq .status                                                                                                                                                        
{
  "clusters": [
    {
      "completedActions": [
        {
          "action": "Prep"
        }
      ],
      "currentAction": {
        "action": "Upgrade"
      },
      "name": "sno2"
    },
    {
      "currentAction": {
        "action": "Prep"
      },
      "name": "sno3"
    }
  ],
  "conditions": [
    {
      "lastTransitionTime": "2025-02-13T09:27:47Z",
      "message": "Waiting for plan step 0 to be completed",
      "reason": "InProgress",
      "status": "True",
      "type": "Progressing"
    }
  ],
  "observedGeneration": 1
}                                                                                                                                                                                                                   -----                                               	

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
[root@localhost active]# oc get jobs,pods -n openshift-lifecycle-agent
NAME                                 STATUS    COMPLETIONS   DURATION   AGE
job.batch/lca-prep-stateroot-setup   Running   0/1           29s        29s

NAME                                                      READY   STATUS    RESTARTS   AGE
pod/lca-prep-stateroot-setup-qkxf7                        1/1     Running   0          29s
pod/lifecycle-agent-controller-manager-7c7f8b9c89-6vv7q   2/2     Running   0          12h
-----

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc get ibgu -n default telco5g-lab -ojson | jq .status                                                                                                                                                        
{
  "clusters": [
    {
      "completedActions": [
        {
          "action": "Prep"
        }
      ],
      "currentAction": {
        "action": "Upgrade"
      },
      "name": "sno2"
    },
    {
      "completedActions": [
        {
          "action": "Prep"
        }
      ],
      "currentAction": {
        "action": "Upgrade"
      },
      "name": "sno3"
    }
  ],
  "conditions": [
    {
      "lastTransitionTime": "2025-02-13T09:27:47Z",
      "message": "Waiting for plan step 0 to be completed",
      "reason": "InProgress",
      "status": "True",
      "type": "Progressing"
    }
  ],
  "observedGeneration": 1
}
-----

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc get ibgu -n default telco5g-lab -ojson | jq .status                                                                                                                                                        
{
  "clusters": [
    {
      "completedActions": [
        {
          "action": "Prep"
        },
	{
          "action": "Upgrade"
        },
	{
          "action": "FinalizeUpgrade"
        }
      ],
      "name": "sno2"
    },
    {
      "completedActions": [
        {
          "action": "Prep"
        }
      ],
      "currentAction": {
        "action": "Upgrade"
      },
      "name": "sno3"
    }
  ],
  "conditions": [
    {
      "lastTransitionTime": "2025-02-13T09:27:47Z",
      "message": "Waiting for plan step 0 to be completed",
      "reason": "InProgress",
      "status": "True",
      "type": "Progressing"
    }
  ],
  "observedGeneration": 1
}
-----

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc get ibgu -n default telco5g-lab -ojson | jq .status                                                                                                                                                        

{
  "clusters": [
    {
      "completedActions": [
        {
          "action": "Prep"
        },
	{
          "action": "Upgrade"
        },
	{
          "action": "FinalizeUpgrade"
        }
      ],
      "name": "sno2"
    },
    {
      "completedActions": [
        {
          "action": "Prep"
        },
	{
          "action": "Upgrade"
        }
      ],
      "currentAction": {
        "action": "FinalizeUpgrade"
      },
      "name": "sno3"
    }
  ],
-----

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc get ibgu -n default telco5g-lab -ojson | jq .status                                                                                                                                                        
{
  "clusters": [
    {
      "completedActions": [
        {
          "action": "Prep"
        },
	      {   
          "action": "Upgrade"
        },
	      {
          "action": "FinalizeUpgrade"
        }
      ],
      "name": "sno2"
    },
    {
      "completedActions": [
        {
          "action": "Prep"
        },
	{
          "action": "Upgrade"
        },
	{
          "action": "FinalizeUpgrade"
        }
      ],
      "name": "sno3"
    }
  ],
  "conditions": [
    {
      "lastTransitionTime": "2025-02-13T09:47:57Z",
      "message": "All plan steps are completed",
      "reason": "Completed",
      "status": "False",
      "type": "Progressing"
    }
  ],
  "observedGeneration": 1
}
-----

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
Every 2.0s: oc get ibgu,cgu -A                                                                                                                                                                             katatonic: Thu Feb 13 10:50:08 2025

NAMESPACE   NAME                                                  AGE
default     imagebasedgroupupgrade.lcm.openshift.io/telco5g-lab   22m

NAMESPACE      NAME                                                                              AGE     STATE       DETAILS
default        clustergroupupgrade.ran.openshift.io/telco5g-lab-prep-upgrade-finalizeupgrade-0   22m     Completed   All manifestworks rolled out successfully on all clusters
ztp-install    clustergroupupgrade.ran.openshift.io/local-cluster                                20h     Completed   All clusters already compliant with the specified managed policies
ztp-install    clustergroupupgrade.ran.openshift.io/sno2                                         6m49s   Completed   All clusters already compliant with the specified managed policies
ztp-install    clustergroupupgrade.ran.openshift.io/sno3                                         3m59s   Completed   All clusters already compliant with the specified managed policies
ztp-policies   clustergroupupgrade.ran.openshift.io/requirements-upgrade                         60m     Completed   All clusters are compliant with all the managed policies
-----


Then enforce the policies again

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
cat <<EOF | oc --kubeconfig ~/5g-deployment-lab/hub-kubeconfig apply -f -
---
apiVersion: ran.openshift.io/v1alpha1
kind: ClusterGroupUpgrade
metadata:
  name: postupgrade-config
  namespace: ztp-policies
spec:
  clusters:
  - sno2
  - sno3
  managedPolicies:
  - common-config-policy
  - common-subscriptions-policy
  - du-sno-group-policy
  - du-sno-sites-sites-policy
  remediationStrategy:
    maxConcurrency: 2
    timeout: 240
EOF
-----


****************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************

[#upgrade-policy-creation]
== Creating the upgrade PGT

Create an upgrade PGT in inform mode, as usual, that will apply and upgrade the SNOs located in Europe (binding rule: `du-zone: "europe"`), SNO2 and SNO3 clusters. This file needs to be created in the ztp-repository Git repo that we have created.

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
cat <<EOF > ~/5g-deployment-lab/ztp-repository/site-policies/fleet/active/{talm-update-file}
---
apiVersion: ran.openshift.io/v1
kind: PolicyGenTemplate
metadata:
  name: "europe-upgrade-idle"
  namespace: "ztp-policies"
spec:
  bindingRules:
    du-zone: "europe"
    logicalGroup: "active"
  mcp: master
  evaluationInterval:
    compliant: 10s
    noncompliant: 10s
  sourceFiles:
    - fileName: reference-crs/ibu/ImageBasedUpgrade.yaml
      policyName: "{talm-update-policy-name}"
      complianceType: mustonlyhave
      spec:
        stage: Idle
        seedImageRef:
          version: "{talm-update-version}"
          image: "{talm-update-image}"
EOF
-----

Modify the kustomization.yaml inside the site-policies folder, so it includes this new PGT and eventually will be applied by ArgoCD.

NOTE: If you're using MacOS and you're getting errors while running `sed -i` commands, make sure you are using `gsed`. If you do not have it available, please install it: `brew install gnu-sed`.

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
sed -i "/- group-du-sno.yaml/a \ \ - requirements-upgrade.yaml \n \ \- {talm-update-file}" ~/5g-deployment-lab/ztp-repository/site-policies/fleet/active/kustomization.yaml
-----

Then commit all the changes:

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
cd ~/5g-deployment-lab/ztp-repository/
git add site-policies/fleet/active/{talm-update-file} site-policies/fleet/active/requirements-upgrade.yaml site-policies/fleet/active/kustomization.yaml
git commit -m "adds upgrade policy"
git push origin main 
-----

Once committed, in a couple of minutes we will see a new policy in the multicloud RHACM console. As noticed, the policy named `europe-snos-upgrade-{talm-update-policy-name}` has a violation. However, only one cluster is not compliant. If we check the policy information we will see that this policy is only targeting SNO2 cluster. That's because the SNO1 cluster does not have the labels zone-europe and active logicalGroup that the PGT is targeting in its binding rule. 

WARNING: Notice in the following picture that there are policies not applying to any cluster. Those are policies targeting the test environment. This is expected since we do not have any clusters in the test environment, e.g, no clusters are labeled with the proper label for testing: logicalGroup=test.

image::talm_upgrade_policy_01.png[TALM upgrade policy 1]

Let's add the proper labels to the production cluster SNO1:

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/5g-deployment-lab/hub-kubeconfig label managedcluster sno3 du-zone=europe logicalGroup=active
-----

[console-input]
[source,console]
-----
managedcluster.cluster.open-cluster-management.io/sno3 labeled
-----

[#upgrade-cgu-creation]
== Applying the upgrade

At this point, we need to create a Cluster Group Upgrade (CGU) resource that will start the upgrade process. In our case, the process will be divided into two stages:

1. Run a pre-cache of the new OCP release prior to start the upgrade process.
2. Before running the upgrade, a backup will be done.

[#talm-backup-precache]
=== Backup and pre-cache

Let's create the CGU. In this case, we will apply the managed policy (europe-snos-upgrade-{talm-update-policy-name}) to both clusters at the same time (maxConcurrency is 2). Notice that the CGU is disabled, this is suggested if we are going to run the precaching feature. This means, that once the precaching process is done we are ready to start the upgrade process by enabling the CGU. This idea is related to the compliance of a maintenance window in an enterprise. 

Remember that several gigabytes of artifacts needs to be downloaded to the spoke for a full upgrade. SNO spoke clusters may have limited bandwidth to the hub cluster hosting the registry, which will make it difficult for the upgrade to complete within the required time. In order to ensure the upgrade can fit within the maintenance window, the required artifacts need to be present on the spoke cluster prior to the upgrade. Therefore, the process is split up into two stages as mentioned.

image::timing.png[TALM maintenance window concept]

In OCP 4.14+, there is a new CRD called `PreCachingConfig` that will allow us to be more precise on the container images that we need for our cluster to upgrade. We must apply the {talm-precachingconfig-doc}[PreCachingConfig CR] before or concurrently with the CGU to our hub cluster:

NOTE: You can obtain a more detailed list for `excludePrecachePattern` for each upgrade by following https://access.redhat.com/articles/7046378[this KCS].

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
cat <<EOF | oc --kubeconfig ~/5g-deployment-lab/hub-kubeconfig apply -f -
---
apiVersion: ran.openshift.io/v1alpha1
kind: PreCachingConfig
metadata:
  name: update-europe-snos
  namespace: ztp-policies
spec:
  overrides: {}
  excludePrecachePatterns: 
    - agent-installer-
    - alibaba-
    - aws-
    - azure-
    - cloud-
    - gcp-
    - ibmcloud
    - ibm-
    - nutanix-
    - openstack-
    - ovirt-
    - powervs-
    - sdn
    - vsphere-
    - kuryr-
    - csi-
    - hypershift
  additionalImages: []
  spaceRequired: 30 GiB
---
apiVersion: ran.openshift.io/v1alpha1
kind: ClusterGroupUpgrade
metadata:
  name: update-ibu-idle 
  namespace: ztp-policies
spec:
  preCaching: true
  preCachingConfigRef:
    name: update-europe-snos
    namespace: ztp-policies
  backup: true
  clusters:
  - sno2
  - sno3
  enable: false
  managedPolicies:
  - lca-install-lca-subscription
  - europe-upgrade-idle-{talm-update-policy-name}
  remediationStrategy:
    maxConcurrency: 2
    timeout: 240
EOF
-----

Once applied, we can see that the status moved to `InProgress` with a message detailing that the precaching process is in progress for both SNOs. This means that the first step in our process is executing the pre-cache.
[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/5g-deployment-lab/hub-kubeconfig get cgu -A
-----

[console-input]
[source,console]
-----
NAMESPACE      NAME                 AGE    STATE        DETAILS
ztp-install    local-cluster        105m   Completed    All clusters already compliant with the specified managed policies
ztp-install    sno1                 23m    Completed    All clusters already compliant with the specified managed policies
ztp-install    sno2                 10m    Completed    All clusters are compliant with all the managed policies
ztp-policies   update-europe-snos   4s     InProgress   Precaching in progress for 2 clusters
-----

Connecting to any of our spoke clusters we can see a new job being created called pre-cache.

WARNING: Pre-cache job can take up to 5m to be created.

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/5g-deployment-lab/sno2-kubeconfig -n openshift-talo-pre-cache get job pre-cache
-----

[console-input]
[source,console]
-----
NAME        COMPLETIONS   DURATION   AGE
pre-cache   0/1           64s        64s
-----

This job creates a Pod that will run the precache process. As we can see below, 183 images need to be downloaded from our local registry to mark the task as successful.

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/5g-deployment-lab/sno2-kubeconfig logs job/pre-cache -n openshift-talo-pre-cache -f
-----

[console-input]
[source,console,subs="attributes+,+macros"]
-----
highThresholdPercent: 85 diskSize:209124332 used:17648032
upgrades.pre-cache {last-update-date}T10:55:10+00:00 DEBUG Release index image processing done
7df5fe3b5fb7352b870735c7d7bd898d0959a9a49558d2ffb42dcd269e01752f
upgrades.pre-cache {last-update-date}T08:13:59+00:00 [DEBUG]: Operators index is not specified. Operators won't be pre-cached
upgrades.pre-cache {last-update-date}T08:13:59+00:00 [INFO]: Image pre-caching starting for platform-images
.
.
.
upgrades.pre-cache {last-update-date}T08:20:36+00:00 [INFO]: Image pre-caching complete for platform-images
-----

Once the precache is done, the CGU state moves to `NotEnabled` and the Pod running the pre-cache task in both SNO clusters is deleted. At this moment, TALM is waiting for acknowledging the start of the upgrade.

WARNING: It can take up to 5 minutes for the CGU to report the new state. After that time, the precache objects and the openshift-talo-pre-cache namespace created in the managed clusters are automatically deleted.

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/5g-deployment-lab/hub-kubeconfig get cgu -n ztp-policies update-europe-snos
-----

[console-input]
[source,console]
-----
NAMESPACE      NAME                 AGE     STATE        DETAILS
ztp-policies   update-europe-snos   29m     NotEnabled   Not enabled
-----

[#talm-upgrade]
=== Triggering the upgrade

Now that the pre-cache is done, we can trigger the update. As we said earlier, before the update is actually executed a backup will be done so we can rollback. In order to trigger the update we need to enable the CGU:

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/5g-deployment-lab/hub-kubeconfig patch cgu update-europe-snos -n ztp-policies --type merge --patch '{"spec":{"enable":true}}'
-----

Notice the CGU state moved to `InProgress`, which means, the upgrade process has started. In the details you can see that the backup is in progress.

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/5g-deployment-lab/hub-kubeconfig get cgu -n ztp-policies update-europe-snos
-----

[console-input]
[source,console]
-----
NAMESPACE      NAME                 AGE     STATE        DETAILS
ztp-policies   update-europe-snos   30m     InProgress   Backup in progress for 2 clusters
-----

Connecting to any of our spoke clusters we can see a new job being created called backup-agent.

WARNING: Backup job can take up to 5m to be created.

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/5g-deployment-lab/sno1-kubeconfig get jobs -A
-----

[console-input]
[source,console]
-----
NAMESPACE                              NAME                            COMPLETIONS   DURATION   AGE
assisted-installer                     assisted-installer-controller   1/1           26m        21h
openshift-image-registry               image-pruner-28140480           1/1           5s         11h
openshift-operator-lifecycle-manager   collect-profiles-28141110       1/1           5s         34m
openshift-operator-lifecycle-manager   collect-profiles-28141125       1/1           4s         19m
openshift-operator-lifecycle-manager   collect-profiles-28141140       1/1           4s         4m50s
openshift-talo-backup                  backup-agent                    0/1           7s         7s
-----

This job basically runs a Pod that will execute a recovery procedure and will store all required data into the /var/recovery folder of each spoke.

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/5g-deployment-lab/sno2-kubeconfig logs job/backup-agent -n openshift-talo-backup -f
-----

[console-input]
[source,console,subs="attributes+,+macros"]
-----
INFO[0000] ------------------------------------------------------------ 
INFO[0000] Cleaning up old content...                   
INFO[0000] ------------------------------------------------------------ 
.
.
.
INFO[0003] ------------------------------------------------------------ 
INFO[0003] backup has successfully finished ...         
-----

Once backups are finished for all clusters, the CGU state will move to `BackupCompleted` and then quickly move to `InProgress`:

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/5g-deployment-lab/hub-kubeconfig get cgu -A
-----

[console-input]
[source,console]
-----
NAMESPACE      NAME                 AGE    STATE             DETAILS
ztp-install    local-cluster        124m   Completed         All clusters already compliant with the specified managed policies
ztp-install    sno1                 42m    Completed         All clusters already compliant with the specified managed policies
ztp-install    sno2                 28m    Completed         All clusters are compliant with all the managed policies
ztp-policies   update-europe-snos   28m    BackupCompleted   Backup is completed for all clusters
-----

At this point, if we connect to any of our spoke clusters we can see that the upgrade process is actually taking place.

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/5g-deployment-lab/sno2-kubeconfig get clusterversion,nodes
-----

[console-input]
[source,console,subs="attributes+,+macros"]
-----
NAME                                         VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
clusterversion.config.openshift.io/version   {sno-cluster-version1-cvo}    True        True          30s     Working towards {sno-cluster-version2-cvo}: 101 of 842 done (11% complete), waiting on kube-apiserver

NAME                          STATUS   ROLES                         AGE    VERSION
node/sno2.5g-deployment.lab   Ready    control-plane,master,worker   156m   {sno-cluster-version2-kubeversion}
-----

Meanwhile, the clusters are upgrading we can take a look at the https://console-openshift-console.apps.hub.5g-deployment.lab/multicloud/governance/policies[multicloud console] and see that there is a new policy in enforce mode:

image::talm_upgrade_policy_03.png[TALM upgrade policy 3]

Moving to the Infrastructure -> Cluster section of the multicloud console we can also graphically see both clusters being upgraded:

image::talm_upgrade_policy_04.png[TALM upgrade policy 3]

Finally, our clusters are upgraded:

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/5g-deployment-lab/sno2-kubeconfig get clusterversion,nodes
-----

[console-input]
[source,console,subs="attributes+,+macros"]
-----
NAME                                         VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
clusterversion.config.openshift.io/version   {sno-cluster-version2-cvo}    True        False         6m8s    Cluster version is {sno-cluster-version2-cvo}

NAME                          STATUS   ROLES                         AGE     VERSION
node/sno2.5g-deployment.lab   Ready    control-plane,master,worker   3h26m   {sno-cluster-version2-kubeversion}
-----

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/5g-deployment-lab/sno1-kubeconfig get clusterversion,nodes
-----

[console-input]
[source,console,subs="attributes+,+macros"]
-----
NAME                                         VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
clusterversion.config.openshift.io/version   {sno-cluster-version2-cvo}    True        False         9m12s   Cluster version is {sno-cluster-version2-cvo}

NAME                      STATUS   ROLES                         AGE   VERSION
node/openshift-master-0   Ready    control-plane,master,worker   22h   {sno-cluster-version2-kubeversion}
-----

Notice that now the upgrade policy `europe-snos-upgrade-{talm-update-policy-name}` is now compliant on both clusters. See that, in order to save resources, the enforce policy is removed once the CGU is successfully applied. 

And finally, the CGU will be `Completed`:

[.console-input]
[source,bash,subs="attributes+,+macros"]
-----
oc --kubeconfig ~/5g-deployment-lab/hub-kubeconfig get cgu -A
-----

[console-input]
[source,console]
-----
NAMESPACE      NAME                 AGE    STATE       DETAILS
ztp-install    local-cluster        23h    Completed   All clusters already compliant with the specified managed policies
ztp-install    sno1                 67m    Completed   All clusters already compliant with the specified managed policies
ztp-install    sno2                 3h5m   Completed   All clusters are compliant with all the managed policies
ztp-policies   update-europe-snos   63m    Completed   All clusters are compliant with all the managed policies
-----

image::talm_upgrade_policy_05.png[TALM upgrade policy 5]
